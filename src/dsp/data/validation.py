"""
Data validation and quality checks for DSP-100K.

Ensures data integrity before use in signal generation
and position sizing calculations.
"""

import logging
from dataclasses import dataclass, field
from datetime import date, timedelta
from enum import Enum
from typing import Any, Dict, List, Optional, Set, Tuple

import numpy as np
import pandas as pd

logger = logging.getLogger(__name__)


class ValidationSeverity(str, Enum):
    """Severity levels for validation issues."""
    INFO = "info"           # Minor issue, informational
    WARNING = "warning"     # Potential problem, proceed with caution
    ERROR = "error"         # Significant issue, may affect results
    CRITICAL = "critical"   # Blocking issue, cannot proceed


@dataclass
class ValidationIssue:
    """A single validation issue."""
    severity: ValidationSeverity
    code: str
    message: str
    symbol: Optional[str] = None
    field: Optional[str] = None
    details: Optional[Dict[str, Any]] = None


@dataclass
class DataQualityReport:
    """
    Comprehensive data quality report.

    Generated by DataValidator after running all checks.
    """
    as_of_date: date
    symbols_checked: int
    rows_checked: int
    issues: List[ValidationIssue] = field(default_factory=list)
    passed: bool = True

    # Summary statistics
    missing_data_pct: float = 0.0
    suspicious_returns_count: int = 0
    stale_data_symbols: List[str] = field(default_factory=list)

    def add_issue(self, issue: ValidationIssue) -> None:
        """Add an issue to the report."""
        self.issues.append(issue)
        if issue.severity in (ValidationSeverity.ERROR, ValidationSeverity.CRITICAL):
            self.passed = False

    @property
    def critical_issues(self) -> List[ValidationIssue]:
        """Get all critical issues."""
        return [i for i in self.issues if i.severity == ValidationSeverity.CRITICAL]

    @property
    def error_issues(self) -> List[ValidationIssue]:
        """Get all error-level issues."""
        return [i for i in self.issues if i.severity == ValidationSeverity.ERROR]

    @property
    def warning_issues(self) -> List[ValidationIssue]:
        """Get all warnings."""
        return [i for i in self.issues if i.severity == ValidationSeverity.WARNING]

    def summary(self) -> str:
        """Get a summary string."""
        status = "✅ PASSED" if self.passed else "❌ FAILED"
        lines = [
            f"Data Quality Report ({self.as_of_date})",
            f"Status: {status}",
            f"Symbols: {self.symbols_checked}, Rows: {self.rows_checked}",
            f"Missing Data: {self.missing_data_pct:.2%}",
            f"Issues: {len(self.critical_issues)} critical, {len(self.error_issues)} errors, {len(self.warning_issues)} warnings",
        ]
        if self.stale_data_symbols:
            lines.append(f"Stale Symbols: {', '.join(self.stale_data_symbols)}")
        return "\n".join(lines)


class DataValidator:
    """
    Validates market data quality.

    Checks for:
    - Missing data (NaN, gaps)
    - Stale data (no recent updates)
    - Suspicious values (extreme returns, negative prices)
    - Data consistency (OHLC relationships)
    """

    # Thresholds for validation
    MAX_DAILY_RETURN = 0.50       # 50% max single-day return
    MIN_DAILY_RETURN = -0.50     # -50% min single-day return
    MAX_MISSING_PCT = 0.05       # 5% max missing data
    STALE_DAYS_THRESHOLD = 3    # Data older than 3 days is stale
    MIN_PRICE = 0.01            # Minimum valid price
    MAX_SPREAD_PCT = 0.10       # 10% max bid-ask spread

    def __init__(
        self,
        max_daily_return: float = MAX_DAILY_RETURN,
        max_missing_pct: float = MAX_MISSING_PCT,
        stale_days: int = STALE_DAYS_THRESHOLD,
    ):
        """
        Initialize validator with custom thresholds.

        Args:
            max_daily_return: Maximum allowed single-day return
            max_missing_pct: Maximum allowed missing data percentage
            stale_days: Number of days before data is considered stale
        """
        self.max_daily_return = max_daily_return
        self.max_missing_pct = max_missing_pct
        self.stale_days = stale_days

    def validate_daily_bars(
        self,
        symbol: str,
        df: pd.DataFrame,
        as_of_date: date,
    ) -> List[ValidationIssue]:
        """
        Validate daily bar data for a single symbol.

        Args:
            symbol: Symbol being validated
            df: DataFrame with OHLCV data
            as_of_date: Reference date for staleness check

        Returns:
            List of validation issues
        """
        issues: List[ValidationIssue] = []

        if df is None or df.empty:
            issues.append(ValidationIssue(
                severity=ValidationSeverity.CRITICAL,
                code="NO_DATA",
                message=f"No data available for {symbol}",
                symbol=symbol,
            ))
            return issues

        # Required columns
        required_cols = ["open", "high", "low", "close", "volume"]
        missing_cols = [c for c in required_cols if c not in df.columns]
        if missing_cols:
            issues.append(ValidationIssue(
                severity=ValidationSeverity.CRITICAL,
                code="MISSING_COLUMNS",
                message=f"Missing required columns: {missing_cols}",
                symbol=symbol,
                details={"missing": missing_cols},
            ))
            return issues

        # Check for missing values
        for col in required_cols:
            missing_pct = df[col].isna().mean()
            if missing_pct > 0:
                severity = (
                    ValidationSeverity.ERROR if missing_pct > self.max_missing_pct
                    else ValidationSeverity.WARNING
                )
                issues.append(ValidationIssue(
                    severity=severity,
                    code="MISSING_VALUES",
                    message=f"{missing_pct:.2%} missing values in {col}",
                    symbol=symbol,
                    field=col,
                    details={"missing_pct": missing_pct},
                ))

        # Check for stale data
        if hasattr(df.index, 'date'):
            last_date = df.index[-1].date() if hasattr(df.index[-1], 'date') else df.index[-1]
        else:
            last_date = df.index[-1]

        if isinstance(last_date, pd.Timestamp):
            last_date = last_date.date()

        days_stale = (as_of_date - last_date).days if isinstance(last_date, date) else 0

        if days_stale > self.stale_days:
            issues.append(ValidationIssue(
                severity=ValidationSeverity.ERROR,
                code="STALE_DATA",
                message=f"Data is {days_stale} days old (threshold: {self.stale_days})",
                symbol=symbol,
                details={"last_date": str(last_date), "days_stale": days_stale},
            ))

        # Check OHLC relationships
        ohlc_issues = self._check_ohlc_relationships(symbol, df)
        issues.extend(ohlc_issues)

        # Check for extreme returns
        return_issues = self._check_returns(symbol, df)
        issues.extend(return_issues)

        # Check for negative/zero prices
        price_issues = self._check_prices(symbol, df)
        issues.extend(price_issues)

        # Check volume
        volume_issues = self._check_volume(symbol, df)
        issues.extend(volume_issues)

        return issues

    def _check_ohlc_relationships(
        self,
        symbol: str,
        df: pd.DataFrame,
    ) -> List[ValidationIssue]:
        """Check that OHLC relationships are valid."""
        issues: List[ValidationIssue] = []

        # High should be >= Low
        invalid_hl = (df["high"] < df["low"]).sum()
        if invalid_hl > 0:
            issues.append(ValidationIssue(
                severity=ValidationSeverity.ERROR,
                code="INVALID_OHLC",
                message=f"High < Low in {invalid_hl} rows",
                symbol=symbol,
                details={"count": int(invalid_hl)},
            ))

        # High should be >= Open, Close
        invalid_high = ((df["high"] < df["open"]) | (df["high"] < df["close"])).sum()
        if invalid_high > 0:
            issues.append(ValidationIssue(
                severity=ValidationSeverity.ERROR,
                code="INVALID_HIGH",
                message=f"High < Open or Close in {invalid_high} rows",
                symbol=symbol,
                details={"count": int(invalid_high)},
            ))

        # Low should be <= Open, Close
        invalid_low = ((df["low"] > df["open"]) | (df["low"] > df["close"])).sum()
        if invalid_low > 0:
            issues.append(ValidationIssue(
                severity=ValidationSeverity.ERROR,
                code="INVALID_LOW",
                message=f"Low > Open or Close in {invalid_low} rows",
                symbol=symbol,
                details={"count": int(invalid_low)},
            ))

        return issues

    def _check_returns(
        self,
        symbol: str,
        df: pd.DataFrame,
    ) -> List[ValidationIssue]:
        """Check for extreme or suspicious returns."""
        issues: List[ValidationIssue] = []

        # Calculate daily returns
        returns = df["close"].pct_change().dropna()

        if len(returns) == 0:
            return issues

        # Check for extreme positive returns
        extreme_pos = returns[returns > self.max_daily_return]
        if len(extreme_pos) > 0:
            issues.append(ValidationIssue(
                severity=ValidationSeverity.WARNING,
                code="EXTREME_RETURN_UP",
                message=f"{len(extreme_pos)} days with return > {self.max_daily_return:.0%}",
                symbol=symbol,
                details={
                    "count": len(extreme_pos),
                    "max_return": float(extreme_pos.max()),
                    "dates": [str(d) for d in extreme_pos.index[:5]],
                },
            ))

        # Check for extreme negative returns
        extreme_neg = returns[returns < self.MIN_DAILY_RETURN]
        if len(extreme_neg) > 0:
            issues.append(ValidationIssue(
                severity=ValidationSeverity.WARNING,
                code="EXTREME_RETURN_DOWN",
                message=f"{len(extreme_neg)} days with return < {self.MIN_DAILY_RETURN:.0%}",
                symbol=symbol,
                details={
                    "count": len(extreme_neg),
                    "min_return": float(extreme_neg.min()),
                    "dates": [str(d) for d in extreme_neg.index[:5]],
                },
            ))

        # Check for zero returns (might indicate stale data)
        zero_returns = (returns == 0).sum()
        zero_pct = zero_returns / len(returns)
        if zero_pct > 0.10:  # More than 10% zero returns is suspicious
            issues.append(ValidationIssue(
                severity=ValidationSeverity.WARNING,
                code="MANY_ZERO_RETURNS",
                message=f"{zero_pct:.1%} of returns are exactly zero",
                symbol=symbol,
                details={"zero_pct": float(zero_pct)},
            ))

        return issues

    def _check_prices(
        self,
        symbol: str,
        df: pd.DataFrame,
    ) -> List[ValidationIssue]:
        """Check for invalid prices."""
        issues: List[ValidationIssue] = []

        for col in ["open", "high", "low", "close"]:
            # Check for negative prices
            negative = (df[col] < 0).sum()
            if negative > 0:
                issues.append(ValidationIssue(
                    severity=ValidationSeverity.CRITICAL,
                    code="NEGATIVE_PRICE",
                    message=f"Negative prices in {col}: {negative} rows",
                    symbol=symbol,
                    field=col,
                    details={"count": int(negative)},
                ))

            # Check for very low prices (might indicate data issue)
            very_low = ((df[col] > 0) & (df[col] < self.MIN_PRICE)).sum()
            if very_low > 0:
                issues.append(ValidationIssue(
                    severity=ValidationSeverity.WARNING,
                    code="VERY_LOW_PRICE",
                    message=f"Very low prices (< ${self.MIN_PRICE}) in {col}: {very_low} rows",
                    symbol=symbol,
                    field=col,
                    details={"count": int(very_low)},
                ))

        return issues

    def _check_volume(
        self,
        symbol: str,
        df: pd.DataFrame,
    ) -> List[ValidationIssue]:
        """Check volume data."""
        issues: List[ValidationIssue] = []

        # Check for negative volume
        negative = (df["volume"] < 0).sum()
        if negative > 0:
            issues.append(ValidationIssue(
                severity=ValidationSeverity.CRITICAL,
                code="NEGATIVE_VOLUME",
                message=f"Negative volume: {negative} rows",
                symbol=symbol,
                field="volume",
                details={"count": int(negative)},
            ))

        # Check for zero volume days
        zero_volume = (df["volume"] == 0).sum()
        zero_pct = zero_volume / len(df)
        if zero_pct > 0.05:  # More than 5% zero volume is suspicious
            issues.append(ValidationIssue(
                severity=ValidationSeverity.WARNING,
                code="MANY_ZERO_VOLUME",
                message=f"{zero_pct:.1%} of days have zero volume",
                symbol=symbol,
                field="volume",
                details={"zero_pct": float(zero_pct)},
            ))

        return issues

    def validate_universe(
        self,
        data: Dict[str, pd.DataFrame],
        as_of_date: date,
        min_history_days: int = 252,
    ) -> DataQualityReport:
        """
        Validate data for an entire universe.

        Args:
            data: Dict mapping symbols to DataFrames
            as_of_date: Reference date
            min_history_days: Minimum required history

        Returns:
            Comprehensive data quality report
        """
        report = DataQualityReport(
            as_of_date=as_of_date,
            symbols_checked=len(data),
            rows_checked=sum(len(df) for df in data.values()),
        )

        total_missing = 0
        total_cells = 0
        suspicious_returns = 0
        stale_symbols: List[str] = []

        for symbol, df in data.items():
            # Run individual symbol validation
            issues = self.validate_daily_bars(symbol, df, as_of_date)
            for issue in issues:
                report.add_issue(issue)

            if df is not None and not df.empty:
                # Track missing data
                for col in ["open", "high", "low", "close", "volume"]:
                    if col in df.columns:
                        total_missing += df[col].isna().sum()
                        total_cells += len(df)

                # Track suspicious returns
                returns = df["close"].pct_change().dropna()
                suspicious = ((returns > self.max_daily_return) |
                             (returns < self.MIN_DAILY_RETURN)).sum()
                suspicious_returns += suspicious

                # Check history length
                if len(df) < min_history_days:
                    report.add_issue(ValidationIssue(
                        severity=ValidationSeverity.WARNING,
                        code="SHORT_HISTORY",
                        message=f"Only {len(df)} days of history (need {min_history_days})",
                        symbol=symbol,
                        details={"days": len(df), "required": min_history_days},
                    ))

                # Track stale symbols
                for issue in issues:
                    if issue.code == "STALE_DATA":
                        stale_symbols.append(symbol)
                        break

        # Calculate summary stats
        report.missing_data_pct = total_missing / total_cells if total_cells > 0 else 0
        report.suspicious_returns_count = suspicious_returns
        report.stale_data_symbols = stale_symbols

        return report

    def validate_returns_matrix(
        self,
        returns: pd.DataFrame,
    ) -> List[ValidationIssue]:
        """
        Validate a returns matrix (symbols as columns).

        Args:
            returns: DataFrame with returns (index=dates, columns=symbols)

        Returns:
            List of validation issues
        """
        issues: List[ValidationIssue] = []

        # Check for NaN values
        nan_pct = returns.isna().sum().sum() / returns.size
        if nan_pct > self.max_missing_pct:
            issues.append(ValidationIssue(
                severity=ValidationSeverity.ERROR,
                code="HIGH_MISSING_RETURNS",
                message=f"{nan_pct:.2%} of returns are missing",
                details={"missing_pct": float(nan_pct)},
            ))

        # Check correlation matrix is positive semi-definite
        corr_matrix = returns.corr()
        eigenvalues = np.linalg.eigvalsh(corr_matrix.fillna(0).values)
        if np.any(eigenvalues < -0.01):  # Allow small numerical errors
            issues.append(ValidationIssue(
                severity=ValidationSeverity.WARNING,
                code="NON_PSD_CORRELATION",
                message="Correlation matrix is not positive semi-definite",
                details={"min_eigenvalue": float(eigenvalues.min())},
            ))

        # Check for perfectly correlated assets
        for i, col1 in enumerate(corr_matrix.columns):
            for col2 in corr_matrix.columns[i+1:]:
                corr = corr_matrix.loc[col1, col2]
                if abs(corr) > 0.99:
                    issues.append(ValidationIssue(
                        severity=ValidationSeverity.WARNING,
                        code="HIGH_CORRELATION",
                        message=f"Very high correlation ({corr:.3f}) between {col1} and {col2}",
                        details={"symbol1": col1, "symbol2": col2, "correlation": float(corr)},
                    ))

        return issues

    def validate_quote(
        self,
        symbol: str,
        bid: float,
        ask: float,
        last: float,
    ) -> List[ValidationIssue]:
        """
        Validate a real-time quote.

        Args:
            symbol: Symbol
            bid: Bid price
            ask: Ask price
            last: Last traded price

        Returns:
            List of validation issues
        """
        issues: List[ValidationIssue] = []

        # Check for crossed market
        if bid > ask:
            issues.append(ValidationIssue(
                severity=ValidationSeverity.ERROR,
                code="CROSSED_MARKET",
                message=f"Bid ({bid}) > Ask ({ask})",
                symbol=symbol,
                details={"bid": bid, "ask": ask},
            ))

        # Check for wide spread
        if ask > 0 and bid > 0:
            spread_pct = (ask - bid) / ((ask + bid) / 2)
            if spread_pct > self.MAX_SPREAD_PCT:
                issues.append(ValidationIssue(
                    severity=ValidationSeverity.WARNING,
                    code="WIDE_SPREAD",
                    message=f"Spread is {spread_pct:.2%} (threshold: {self.MAX_SPREAD_PCT:.0%})",
                    symbol=symbol,
                    details={"spread_pct": float(spread_pct)},
                ))

        # Check last price is within bid-ask
        if bid > 0 and ask > 0 and last > 0:
            if last < bid * 0.95 or last > ask * 1.05:
                issues.append(ValidationIssue(
                    severity=ValidationSeverity.WARNING,
                    code="LAST_OUTSIDE_SPREAD",
                    message=f"Last ({last}) is outside bid-ask ({bid}-{ask})",
                    symbol=symbol,
                    details={"bid": bid, "ask": ask, "last": last},
                ))

        # Check for zero/negative prices
        for name, price in [("bid", bid), ("ask", ask), ("last", last)]:
            if price < 0:
                issues.append(ValidationIssue(
                    severity=ValidationSeverity.CRITICAL,
                    code="NEGATIVE_QUOTE_PRICE",
                    message=f"Negative {name} price: {price}",
                    symbol=symbol,
                    field=name,
                ))

        return issues
